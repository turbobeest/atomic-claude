# =============================================================================
# MODELS REGISTRY - Global LLM Capability Scores and Selection Profiles
# =============================================================================
# This registry defines capability scores for all available LLMs.
# Agents reference this registry via model_selection priorities.
# The orchestrator uses these scores + runtime constraints to select models.
#
# Scores are 1-10 scale:
#   - quality: Overall output quality and accuracy
#   - speed: Response latency (10=fastest)
#   - cost: Affordability (1=expensive, 10=free)
#   - code_generation: New code synthesis ability
#   - code_debugging: Bug fixing and SWE-bench style tasks
#   - reasoning: Complex multi-step logical reasoning
#   - math: Mathematical problem solving
#   - tool_use: Function calling and API integration
#   - writing: Prose quality for documentation
# =============================================================================

models:
  # ===========================================================================
  # FRONTIER TIER - Cloud API Models
  # ===========================================================================

  Opus4.5:
    type: cloud
    provider: anthropic
    quality: 10
    speed: 7
    cost: 2
    code_generation: 9
    code_debugging: 10
    reasoning: 10
    math: 9
    tool_use: 10
    writing: 10
    context_window: 200000
    swe_bench: 80.9
    notes: "Best for complex debugging, security-critical, novel problems, PhD-tier reasoning"

  Sonnet4.5:
    type: cloud
    provider: anthropic
    quality: 9
    speed: 8
    cost: 4
    code_generation: 9
    code_debugging: 9
    reasoning: 9
    math: 8
    tool_use: 9
    writing: 9
    context_window: 200000
    swe_bench: 82.0  # agentic
    notes: "Best balance of quality/speed/cost, strong agentic coding"

  Haiku4.5:
    type: cloud
    provider: anthropic
    quality: 7
    speed: 10
    cost: 8
    code_generation: 7
    code_debugging: 6
    reasoning: 7
    math: 6
    tool_use: 7
    writing: 7
    context_window: 200000
    notes: "Fast exploration, simple tasks, cost-effective for high volume"

  GLM-4.7:
    type: cloud
    provider: zhipu
    quality: 9
    speed: 8
    cost: 7
    code_generation: 9
    code_debugging: 7
    reasoning: 8
    math: 10
    tool_use: 10
    writing: 8
    context_window: 128000
    aime_2025: 95.7
    notes: "Excellent for math, code synthesis, tool use. Beats Opus on math benchmarks."

  # ===========================================================================
  # ELITE OPEN SOURCE - Top-tier local/self-hosted models
  # ===========================================================================

  DeepSeek-V3:
    type: local
    provider: deepseek
    quality: 9
    speed: 6
    cost: 10
    code_generation: 9
    code_debugging: 8
    reasoning: 9
    math: 10
    tool_use: 8
    writing: 8
    context_window: 128000
    size_class: large
    parameters: 671B MoE (37B active)
    mmlu: 88.5
    math_500: 90.2
    notes: "GPT-4.5-surpassing on math/reasoning, excellent cost efficiency"

  Qwen3-235B-A22B:
    type: local
    provider: alibaba
    quality: 9
    speed: 5
    cost: 10
    code_generation: 9
    code_debugging: 8
    reasoning: 9
    math: 9
    tool_use: 8
    writing: 9
    context_window: 32000
    size_class: large
    parameters: 235B MoE
    livecode_bench: 69.5
    notes: "Top multilingual (119 languages), flexible reasoning modes"

  Qwen2.5-Coder-32B:
    type: local
    provider: alibaba
    quality: 8
    speed: 7
    cost: 10
    code_generation: 10
    code_debugging: 8
    reasoning: 7
    math: 7
    tool_use: 7
    writing: 6
    context_window: 32000
    size_class: medium
    parameters: 32B
    notes: "Best open-source pure code model, competitive with closed models"

  Kimi-K2-Thinking:
    type: local
    provider: moonshot
    quality: 9
    speed: 5
    cost: 10
    code_generation: 9
    code_debugging: 9
    reasoning: 10
    math: 9
    tool_use: 8
    writing: 8
    context_window: 128000
    size_class: large
    notes: "Arguably best open model by benchmarks, strong reasoning"

  # ===========================================================================
  # STRONG LOCAL - 70B+ parameter models
  # ===========================================================================

  mistral-large:123b:
    type: local
    provider: mistral
    quality: 8
    speed: 4
    cost: 10
    code_generation: 8
    code_debugging: 7
    reasoning: 8
    math: 7
    tool_use: 7
    writing: 8
    context_window: 128000
    size_class: large
    parameters: 123B
    notes: "Strong general reasoning, slow on consumer hardware"

  nemotron:70b:
    type: local
    provider: nvidia
    quality: 8
    speed: 5
    cost: 10
    code_generation: 7
    code_debugging: 7
    reasoning: 8
    math: 7
    tool_use: 7
    writing: 7
    context_window: 32000
    size_class: large
    parameters: 70B
    notes: "NVIDIA optimized, good reasoning"

  llama3.3:70b:
    type: local
    provider: meta
    quality: 8
    speed: 5
    cost: 10
    code_generation: 8
    code_debugging: 7
    reasoning: 8
    math: 7
    tool_use: 7
    writing: 8
    context_window: 128000
    size_class: large
    parameters: 70B
    notes: "Latest Llama, improved over 3.1"

  llama3.1:70b:
    type: local
    provider: meta
    quality: 7
    speed: 5
    cost: 10
    code_generation: 7
    code_debugging: 7
    reasoning: 7
    math: 7
    tool_use: 7
    writing: 7
    context_window: 128000
    size_class: large
    parameters: 70B
    notes: "Proven, reliable, widely supported"

  # ===========================================================================
  # EFFICIENT LOCAL - 27-35B parameter models
  # ===========================================================================

  command-r:35b:
    type: local
    provider: cohere
    quality: 7
    speed: 7
    cost: 10
    code_generation: 6
    code_debugging: 6
    reasoning: 7
    math: 6
    tool_use: 7
    writing: 8
    context_window: 128000
    size_class: medium
    parameters: 35B
    notes: "Excellent for RAG, content generation, retrieval tasks"

  granite-code:34b:
    type: local
    provider: ibm
    quality: 7
    speed: 7
    cost: 10
    code_generation: 8
    code_debugging: 7
    reasoning: 6
    math: 6
    tool_use: 6
    writing: 5
    context_window: 8000
    size_class: medium
    parameters: 34B
    notes: "IBM code specialist, enterprise-focused"

  gemma3:27b:
    type: local
    provider: google
    quality: 7
    speed: 7
    cost: 10
    code_generation: 7
    code_debugging: 6
    reasoning: 7
    math: 6
    tool_use: 6
    writing: 7
    context_window: 8000
    size_class: medium
    parameters: 27B
    notes: "Google balanced model, good general capability"

  # ===========================================================================
  # CODE SPECIALISTS - Purpose-built for development
  # ===========================================================================

  devstral-2:
    type: local
    provider: mistral
    quality: 7
    speed: 7
    cost: 10
    code_generation: 9
    code_debugging: 7
    reasoning: 6
    math: 5
    tool_use: 6
    writing: 5
    context_window: 32000
    size_class: medium
    notes: "Mistral's development-focused model"

  codestral:
    type: local
    provider: mistral
    quality: 7
    speed: 7
    cost: 10
    code_generation: 9
    code_debugging: 7
    reasoning: 6
    math: 6
    tool_use: 6
    writing: 5
    context_window: 32000
    size_class: medium
    notes: "Mistral code specialist"

  phi4-reasoning:
    type: local
    provider: microsoft
    quality: 7
    speed: 8
    cost: 10
    code_generation: 7
    code_debugging: 6
    reasoning: 8
    math: 8
    tool_use: 6
    writing: 6
    context_window: 16000
    size_class: small
    parameters: ~14B
    notes: "Microsoft reasoning specialist, efficient size"

  starcoder2:15b:
    type: local
    provider: bigcode
    quality: 6
    speed: 8
    cost: 10
    code_generation: 8
    code_debugging: 6
    reasoning: 5
    math: 5
    tool_use: 5
    writing: 4
    context_window: 16000
    size_class: small
    parameters: 15B
    notes: "Code-trained, efficient for completion tasks"

  devstral-small-2:
    type: local
    provider: mistral
    quality: 6
    speed: 9
    cost: 10
    code_generation: 7
    code_debugging: 5
    reasoning: 5
    math: 4
    tool_use: 5
    writing: 4
    context_window: 32000
    size_class: small
    notes: "Smaller Mistral dev model, faster"

  codegemma:7b:
    type: local
    provider: google
    quality: 5
    speed: 9
    cost: 10
    code_generation: 7
    code_debugging: 5
    reasoning: 4
    math: 4
    tool_use: 4
    writing: 3
    context_window: 8000
    size_class: small
    parameters: 7B
    notes: "Small code specialist, good for quick completions"

  # ===========================================================================
  # FAST/EDGE - Lightweight models for speed or edge deployment
  # ===========================================================================

  llama3.2:
    type: local
    provider: meta
    quality: 5
    speed: 10
    cost: 10
    code_generation: 5
    code_debugging: 4
    reasoning: 5
    math: 4
    tool_use: 4
    writing: 5
    context_window: 128000
    size_class: tiny
    parameters: 3B/1B
    notes: "Fast, lightweight, good for simple tasks"

  Nemotron-Mini-4B-Instruct:
    type: local
    provider: nvidia
    quality: 4
    speed: 10
    cost: 10
    code_generation: 4
    code_debugging: 3
    reasoning: 4
    math: 4
    tool_use: 3
    writing: 4
    context_window: 4000
    size_class: tiny
    parameters: 4B
    notes: "Very fast, edge deployment"

  Nemotron-3-Nano:
    type: local
    provider: nvidia
    quality: 3
    speed: 10
    cost: 10
    code_generation: 3
    code_debugging: 2
    reasoning: 3
    math: 3
    tool_use: 2
    writing: 3
    context_window: 4000
    size_class: tiny
    notes: "Smallest, edge-only deployment"

# =============================================================================
# SELECTION PROFILES - Predefined strategies for model selection
# =============================================================================
# Agents specify which profile(s) apply to them.
# Orchestrator uses profile + available models to select best option.

selection_profiles:
  quality_critical:
    description: "Maximum accuracy, no compromises. For security-critical, novel problems, PhD-tier reasoning."
    sort_by: [quality, reasoning, code_debugging]
    minimum_quality: 8
    preferred_models: [Opus4.5, Sonnet4.5, GLM-4.7, DeepSeek-V3]

  interactive:
    description: "User waiting for response. Balance speed with quality."
    sort_by: [speed, quality]
    prefer_type: cloud
    minimum_quality: 6
    preferred_models: [Sonnet4.5, Haiku4.5, GLM-4.7]

  batch:
    description: "Overnight or background jobs. Prioritize cost, then quality."
    sort_by: [cost, quality]
    prefer_type: local
    minimum_quality: 6
    preferred_models: [DeepSeek-V3, Qwen3-235B-A22B, llama3.3:70b]

  code_generation:
    description: "Writing new code. Prioritize code synthesis ability."
    sort_by: [code_generation, quality, speed]
    minimum_quality: 6
    preferred_models: [Sonnet4.5, Qwen2.5-Coder-32B, devstral-2, codestral]

  code_review:
    description: "Reviewing and debugging existing code. Prioritize debugging accuracy."
    sort_by: [code_debugging, quality, reasoning]
    minimum_quality: 7
    preferred_models: [Opus4.5, Sonnet4.5, DeepSeek-V3]

  math_reasoning:
    description: "Mathematical and logical tasks. Prioritize math ability."
    sort_by: [math, reasoning, quality]
    minimum_quality: 7
    preferred_models: [GLM-4.7, DeepSeek-V3, Opus4.5, phi4-reasoning]

  documentation:
    description: "Writing documentation, tutorials, content. Prioritize writing quality."
    sort_by: [writing, quality, speed]
    minimum_quality: 6
    preferred_models: [Sonnet4.5, Opus4.5, command-r:35b, llama3.3:70b]

  budget:
    description: "Minimize cost while maintaining acceptable quality."
    sort_by: [cost, speed, quality]
    prefer_type: local
    minimum_quality: 5
    preferred_models: [llama3.3:70b, Qwen2.5-Coder-32B, gemma3:27b]

  exploration:
    description: "Quick exploration, prototyping, simple tasks. Speed is priority."
    sort_by: [speed, cost]
    minimum_quality: 4
    preferred_models: [Haiku4.5, llama3.2, codegemma:7b]

  security_audit:
    description: "Security-critical analysis. Maximum accuracy required."
    sort_by: [quality, code_debugging, reasoning]
    minimum_quality: 9
    preferred_models: [Opus4.5, Sonnet4.5]

# =============================================================================
# SIZE CLASS DEFINITIONS
# =============================================================================
# Used for minimum_tier constraints in agent model_selection

size_classes:
  large:
    description: "70B+ parameters, requires significant GPU memory"
    models: [Opus4.5, Sonnet4.5, GLM-4.7, DeepSeek-V3, Qwen3-235B-A22B, mistral-large:123b, nemotron:70b, llama3.3:70b, llama3.1:70b, Kimi-K2-Thinking]

  medium:
    description: "27-35B parameters, runs on consumer GPUs"
    models: [Haiku4.5, Qwen2.5-Coder-32B, command-r:35b, granite-code:34b, gemma3:27b, devstral-2, codestral]

  small:
    description: "7-15B parameters, fast inference"
    models: [phi4-reasoning, starcoder2:15b, devstral-small-2, codegemma:7b]

  tiny:
    description: "<7B parameters, edge deployment"
    models: [llama3.2, Nemotron-Mini-4B-Instruct, Nemotron-3-Nano]
