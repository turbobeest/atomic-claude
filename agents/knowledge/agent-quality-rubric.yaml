# =============================================================================
# AGENT QUALITY RUBRIC - Machine-Readable Scoring Criteria
# =============================================================================
# Purpose: Define objective criteria for evaluating agent quality across 10 dimensions
# Used by: agent-linter, agent-quality-auditor, audit-report-generator
# Version: 1.0.0
# =============================================================================

version: "1.0.0"
passing_threshold: 80  # Grade B minimum for production-ready

# -----------------------------------------------------------------------------
# GRADE DEFINITIONS
# -----------------------------------------------------------------------------
grades:
  A:
    range: [90, 100]
    label: "Exemplary"
    status: "No action needed"
  B:
    range: [80, 89]
    label: "Production Ready"
    status: "Minimum threshold met"
  C:
    range: [70, 79]
    label: "Needs Improvement"
    status: "Targeted fixes required"
  D:
    range: [60, 69]
    label: "Significant Work Required"
    status: "Major revision needed"
  F:
    range: [0, 59]
    label: "Requires Rewrite"
    status: "Fundamental restructuring"

# -----------------------------------------------------------------------------
# REMEDIATION PRIORITIES
# -----------------------------------------------------------------------------
priority_tiers:
  P0:
    threshold: 50
    action: "Immediate rewrite"
    urgency: "critical"
  P1:
    threshold: 70
    action: "Major fixes required"
    urgency: "high"
  P2:
    threshold: 80
    action: "Targeted improvements to reach Grade B"
    urgency: "medium"
  P3:
    threshold: 90
    action: "Minor polish"
    urgency: "low"
  P4:
    threshold: 100
    action: "Exemplary, no action needed"
    urgency: "none"

# -----------------------------------------------------------------------------
# DIMENSION 1: STRUCTURAL COMPLETENESS (10%)
# Evaluation: Automated - binary presence checks
# -----------------------------------------------------------------------------
dimensions:
  structural_completeness:
    weight: 10
    evaluation_type: automated
    description: "Check presence and format of required sections"

    required_sections:
      frontmatter:
        required: true
        description: "YAML frontmatter between --- markers"
        score_if_missing: 0

      identity:
        required: true
        description: "Identity section with persona and lens"
        must_contain:
          - "interpretive lens"
          - "vocabulary"
        score_if_missing: 0

      instructions:
        required: true
        description: "Instructions section with Always and mode-specific"
        subsections:
          - "Always"
          - "When Generative|When Critical|When Evaluative|When Informative"
        score_if_missing: 0

      never:
        required: true
        description: "Never section with anti-patterns"
        min_items: 3
        score_if_missing: 0

      specializations:
        required_for: [expert, phd]
        description: "Domain specializations with expertise depth"
        score_if_missing: 0

      knowledge_sources:
        required_for: [expert, phd]
        description: "Authoritative references and resources"
        score_if_missing: 0

      output_format:
        required: true
        description: "Output format with envelope and mode-specific templates"
        must_contain:
          - "Result"
          - "Confidence"
        score_if_missing: 0

    scoring:
      method: "percentage_present"
      formula: "(present_sections / required_sections) * 100"

  # ---------------------------------------------------------------------------
  # DIMENSION 2: TIER ALIGNMENT (15%)
  # Evaluation: Automated - token and instruction count validation
  # ---------------------------------------------------------------------------
  tier_alignment:
    weight: 15
    evaluation_type: automated
    description: "Validate agent matches declared tier specifications"

    tier_targets:
      focused:
        token_target: 500
        token_tolerance: 0.20  # Â±20%
        instruction_range: [5, 10]
        allowed_models: [sonnet, haiku]
        required_model: null

      expert:
        token_target: 1500
        token_tolerance: 0.20
        instruction_range: [15, 20]
        allowed_models: [sonnet, opus]
        required_model: null

      phd:
        token_target: 3000
        token_tolerance: 0.20
        instruction_range: [25, 35]
        allowed_models: [opus]
        required_model: opus

    checks:
      token_count:
        weight: 40
        scoring: "100 - (deviation_percentage * 2)"
        max_penalty: 40

      instruction_count:
        weight: 30
        scoring: "100 if in_range else 100 - (distance_from_range * 5)"
        max_penalty: 30

      model_assignment:
        weight: 30
        scoring: "100 if valid else 0"
        critical: true  # PhD with non-opus = automatic fail

  # ---------------------------------------------------------------------------
  # DIMENSION 3: INSTRUCTION QUALITY (15%)
  # Evaluation: Agent - qualitative assessment by auditor
  # ---------------------------------------------------------------------------
  instruction_quality:
    weight: 15
    evaluation_type: agent
    description: "Evaluate instruction domain specificity, actionability, and priority"

    criteria:
      domain_specificity:
        weight: 30
        description: "Instruction is unique to this domain, not generic"
        scoring_guide:
          10: "Highly specific to domain, couldn't apply elsewhere"
          7: "Domain-relevant with some generic elements"
          4: "Mostly generic with domain keywords added"
          0: "Completely generic advice"

        anti_patterns:
          - "be thorough"
          - "write clean code"
          - "follow best practices"
          - "ensure quality"
          - "be helpful"
          - "provide clear explanations"

      actionability:
        weight: 30
        description: "Instruction can be followed and compliance verified"
        scoring_guide:
          10: "Clear action with verifiable outcome"
          7: "Action clear, verification somewhat subjective"
          4: "Vague action, difficult to verify"
          0: "No clear action or completely subjective"

        anti_patterns:
          - "consider"
          - "think about"
          - "be aware of"
          - "keep in mind"

      priority_alignment:
        weight: 20
        description: "Most important instructions appear first"
        scoring_guide:
          10: "Critical domain behaviors in Always section"
          7: "Good priority ordering with minor issues"
          4: "Important instructions buried in mode-specific"
          0: "No apparent priority logic"

      no_redundancy:
        weight: 20
        description: "No duplicate or overlapping instructions"
        scoring_guide:
          10: "Each instruction addresses unique behavior"
          7: "Minor overlaps between sections"
          4: "Significant redundancy across sections"
          0: "Multiple duplicate instructions"

  # ---------------------------------------------------------------------------
  # DIMENSION 4: VOCABULARY CALIBRATION (10%)
  # Evaluation: Automated + Agent
  # ---------------------------------------------------------------------------
  vocabulary_calibration:
    weight: 10
    evaluation_type: hybrid
    description: "Validate vocabulary count and domain specificity"

    automated_checks:
      term_count:
        target: [15, 20]
        weight: 40
        scoring: "100 if in_range else 100 - (distance * 5)"

      terms_present:
        weight: 20
        description: "Terms appear in Vocabulary field"
        scoring: "percentage_present * 100"

    agent_checks:
      domain_specificity:
        weight: 25
        description: "Terms are specific to domain, not generic"
        scoring_guide:
          10: "All terms are domain jargon or technical terminology"
          7: "Most terms specific, some general"
          4: "Mix of specific and generic"
          0: "Mostly generic terms"

      authority_signal:
        weight: 15
        description: "Terms establish expertise boundaries"
        scoring_guide:
          10: "Terms signal deep domain expertise"
          7: "Terms indicate competence"
          4: "Terms are surface-level"
          0: "Terms don't differentiate from non-expert"

  # ---------------------------------------------------------------------------
  # DIMENSION 5: KNOWLEDGE SOURCE AUTHORITY (15%)
  # Evaluation: Agent - assess source quality
  # ---------------------------------------------------------------------------
  knowledge_source_authority:
    weight: 15
    evaluation_type: agent
    description: "Evaluate authority and relevance of knowledge sources"

    source_quality_scores:
      official_documentation: 10
      rfc_standards_bodies: 10
      academic_papers: 9
      recognized_experts: 8
      community_best_practices: 6
      technical_blogs: 4
      generic_blogs: 3
      missing_sources: 0

    quantity_requirements:
      focused:
        minimum: 1
        target: 2
      expert:
        minimum: 3
        target: 5
      phd:
        minimum: 5
        target: 8

    validation_checks:
      url_validity:
        weight: 20
        description: "URLs are properly formatted"

      domain_match:
        weight: 30
        description: "Sources match agent's domain"

      source_diversity:
        weight: 20
        description: "Mix of source types (docs, papers, standards)"

      recency:
        weight: 30
        description: "Sources are current and maintained"

  # ---------------------------------------------------------------------------
  # DIMENSION 6: IDENTITY CLARITY (10%)
  # Evaluation: Agent - assess persona and lens quality
  # ---------------------------------------------------------------------------
  identity_clarity:
    weight: 10
    evaluation_type: agent
    description: "Evaluate persona specificity and interpretive lens clarity"

    components:
      persona_specificity:
        weight: 35
        description: "Persona is specific, not generic 'helpful assistant'"
        scoring_guide:
          10: "Distinct persona with clear expertise domain"
          7: "Clear role but somewhat generic framing"
          4: "Generic expert description"
          0: "No distinct persona or 'helpful assistant'"

        anti_patterns:
          - "helpful assistant"
          - "knowledgeable expert"
          - "experienced professional"

      interpretive_lens:
        weight: 35
        description: "Clear statement of how agent interprets problems"
        scoring_guide:
          10: "Explicit lens statement with unique perspective"
          7: "Implicit lens, derivable from context"
          4: "Vague or generic perspective"
          0: "No discernible interpretive frame"

        positive_patterns:
          - "interpret.*through.*lens"
          - "view.*through"
          - "approach.*from perspective"

      domain_boundaries:
        weight: 30
        description: "Clear what's in/out of scope"
        scoring_guide:
          10: "Explicit scope boundaries stated"
          7: "Scope implicit from specializations"
          4: "Scope unclear, could overlap with others"
          0: "No scope definition"

  # ---------------------------------------------------------------------------
  # DIMENSION 7: ANTI-PATTERN SPECIFICITY (5%)
  # Evaluation: Agent - assess Never section quality
  # ---------------------------------------------------------------------------
  anti_pattern_specificity:
    weight: 5
    evaluation_type: agent
    description: "Evaluate specificity of Never section items"

    criteria:
      specificity:
        weight: 50
        description: "Items are specific failure modes, not vague"
        scoring_guide:
          10: "All items are specific, domain-relevant failure modes"
          7: "Most items specific, one or two vague"
          4: "Mix of specific and vague"
          0: "All items are vague or generic"

        anti_patterns:
          - "never be unhelpful"
          - "never make mistakes"
          - "never provide wrong answers"
          - "never be inaccurate"

      actionability:
        weight: 50
        description: "Items can be violated or complied with"
        scoring_guide:
          10: "All items have clear compliance criteria"
          7: "Most items testable"
          4: "Some items subjective"
          0: "Items cannot be verified"

  # ---------------------------------------------------------------------------
  # DIMENSION 8: OUTPUT FORMAT COMPLIANCE (5%)
  # Evaluation: Automated - check required elements
  # ---------------------------------------------------------------------------
  output_format_compliance:
    weight: 5
    evaluation_type: automated
    description: "Check output format includes required elements"

    required_elements:
      result_section:
        required: true
        pattern: "Result"
        weight: 25

      confidence_indicator:
        required: true
        pattern: "Confidence.*high.*medium.*low"
        weight: 25

      uncertainty_factors:
        required: true
        pattern: "Uncertainty|Uncertainty Factors"
        weight: 20

      verification_steps:
        required: true
        pattern: "Verification"
        weight: 20

      mode_specific_formats:
        required: false
        patterns:
          - "Audit Mode|When Auditing"
          - "Solution Mode|When Generative"
        weight: 10

    scoring:
      method: "weighted_presence"

  # ---------------------------------------------------------------------------
  # DIMENSION 9: FRONTMATTER COMPLETENESS (5%)
  # Evaluation: Automated - check required fields
  # ---------------------------------------------------------------------------
  frontmatter_completeness:
    weight: 5
    evaluation_type: automated
    description: "Validate frontmatter contains required and optional fields"

    required_fields:
      name:
        weight: 15
        validation: "non_empty_string"
      description:
        weight: 15
        validation: "non_empty_string"
      model:
        weight: 15
        validation: "enum:sonnet,opus,haiku"
      tier:
        weight: 15
        validation: "enum:focused,expert,phd"
      tools:
        weight: 10
        validation: "object_with_keys"
      cognitive_modes:
        weight: 10
        validation: "object_with_default"
      role:
        weight: 10
        validation: "enum:executor,auditor,advisor,validator"
      version:
        weight: 10
        validation: "semver"

    optional_bonus_fields:
      model_selection:
        bonus: 5
        description: "Priority-based model selection"
      escalation:
        bonus: 5
        description: "Escalation triggers and thresholds"
      ensemble_roles:
        bonus: 5
        description: "Multi-agent behavior adaptation"
      proactive_triggers:
        bonus: 3
        description: "Auto-invocation patterns"
      load_bearing:
        bonus: 2
        description: "Critical path indicator"

  # ---------------------------------------------------------------------------
  # DIMENSION 10: CROSS-AGENT CONSISTENCY (10%)
  # Evaluation: Automated + Agent
  # ---------------------------------------------------------------------------
  cross_agent_consistency:
    weight: 10
    evaluation_type: hybrid
    description: "Validate consistency with category peers"

    automated_checks:
      section_ordering:
        weight: 25
        expected_order:
          - "Identity"
          - "Core Principles"
          - "Instructions"
          - "Never"
          - "Specializations"
          - "Knowledge Sources"
          - "Output"

      naming_conventions:
        weight: 25
        patterns:
          file_name: "^[a-z][a-z0-9-]*\\.md$"
          agent_name: "^[a-z][a-z0-9-]*$"

    agent_checks:
      role_differentiation:
        weight: 25
        description: "No duplicate coverage with category peers"
        scoring_guide:
          10: "Unique role, clear differentiation"
          7: "Minor overlaps with clear primary"
          4: "Significant overlap with peers"
          0: "Duplicate of existing agent"

      tier_appropriateness:
        weight: 25
        description: "Tier matches complexity relative to peers"
        scoring_guide:
          10: "Tier clearly appropriate"
          7: "Tier reasonable with minor questions"
          4: "Tier seems misaligned"
          0: "Tier clearly wrong"

# -----------------------------------------------------------------------------
# COMPOSITE SCORING
# -----------------------------------------------------------------------------
composite_scoring:
  method: "weighted_average"

  calculation: |
    composite = sum(dimension.score * dimension.weight for dimension in dimensions) / 100

  adjustments:
    critical_failure_penalty:
      conditions:
        - "PhD tier with non-opus model"
        - "Missing frontmatter entirely"
        - "No instructions section"
      penalty: "Cap score at 40%"

    exemplary_bonus:
      conditions:
        - "All automated checks pass"
        - "All agent evaluations >= 8"
      bonus: "+5% (max 100%)"

# -----------------------------------------------------------------------------
# CALIBRATION BENCHMARKS
# -----------------------------------------------------------------------------
calibration:
  known_high_quality:
    - path: "expert-agents/backend-ecosystems/application-languages/typescript-pro.md"
      expected_score_range: [85, 95]
    - path: "expert-agents/frontend-ecosystems/javascript-frameworks/reactjs-expert.md"
      expected_score_range: [85, 95]
    - path: "pipeline-agents/-dev-system/03-05-validation-planning/prd-validator.md"
      expected_score_range: [85, 95]

  known_improvement_needed:
    - path: "expert-agents/development-tooling/code-quality/legacy-modernizer.md"
      expected_score_range: [60, 75]

  validation:
    description: "High-quality agents should score 85%+, weak agents lower"
    tolerance: 10  # Points of acceptable deviation
